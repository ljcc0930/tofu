{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhilif/anaconda3/envs/tf/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os, sys\n",
    "from os.path import join\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from scipy.stats import sem\n",
    "\n",
    "import seaborn as sns\n",
    "import copy\n",
    "from natsort import natsorted\n",
    "import os\n",
    "from os.path import join \n",
    "from pathlib import Path\n",
    "from scipy.stats import ks_2samp\n",
    "from matplotlib import font_manager\n",
    "from matplotlib.font_manager import FontProperties\n",
    "font_path = '../Times New Roman.ttf'\n",
    "prop = FontProperties(fname=font_path)\n",
    "plt.rcParams['font.family'] = prop.get_name()\n",
    "# add font to font manager \n",
    "font_manager.fontManager.addfont(font_path)\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.5, font=prop.get_name())\n",
    "\n",
    "colors = list(sns.color_palette(\"magma\", n_colors=8))\n",
    "fs=22\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_df(df):\n",
    "    mean_df = copy.deepcopy(df)\n",
    "    # delete row \"generated_text\"\n",
    "    mean_df = mean_df.drop(['generated_text', 'normalized_gt_loss'], axis=0)\n",
    "    mean_df.loc['avg_gt_prob']=np.zeros(len(mean_df.columns))\n",
    "    # for each entry in df, the entry is a dict, get the mean of the values\n",
    "    for eval_task, metrics in mean_df.items():\n",
    "        # iterate through metrics\n",
    "        for metric, res in metrics.items():\n",
    "            # get mean\n",
    "            # print(metric)\n",
    "            if metric in ['avg_gt_prob', 'forget_quality', 'truth_ratio']:\n",
    "                continue\n",
    "            mean_df[eval_task][metric] = np.mean(list(res.values()))\n",
    "\n",
    "        if 'eval_log' in eval_task:\n",
    "            perplexities = np.array(list(df[eval_task]['avg_gt_loss'].values()))\n",
    "            probs = np.exp(-1 * perplexities)\n",
    "            mean_df[eval_task]['avg_gt_prob'] = np.mean(probs)\n",
    "\n",
    "        else:\n",
    "            avg_gt_loss = df[eval_task]['avg_gt_loss']\n",
    "            avg_perturb_loss = df[eval_task]['average_perturb_loss']\n",
    "            data_indices = avg_gt_loss.keys()\n",
    "            normalized_gt_prob = {}\n",
    "            for idx in data_indices:\n",
    "                truth_prob = np.exp(-1 * avg_gt_loss[idx])\n",
    "                perturb_prob = np.exp(-1 * np.array(avg_perturb_loss[idx]))\n",
    "                all_prob = np.array([truth_prob, *perturb_prob])\n",
    "                normalized_gt_prob[idx] = truth_prob / all_prob.sum()\n",
    "            mean_df[eval_task]['avg_gt_prob'] = np.mean(np.array(list(normalized_gt_prob.values())))\n",
    "        \n",
    "        if eval_task == 'eval_log_forget.json':\n",
    "            # truth_ratio = np.array(list(df[eval_task]['truth_ratio'].values()))\n",
    "            # adjusted_truth_ratio = np.minimum(truth_ratio, 1/truth_ratio)\n",
    "\n",
    "            avg_paraphrased_loss = df[eval_task]['avg_paraphrased_loss']\n",
    "            avg_perturb_loss = df[eval_task]['average_perturb_loss']\n",
    "\n",
    "            data_indices = list(avg_paraphrased_loss.keys())\n",
    "            avg_paraphrased_loss = np.array([avg_paraphrased_loss[idx] for idx in data_indices])\n",
    "            avg_perturb_loss = np.array([avg_perturb_loss[idx] for idx in data_indices]).mean(-1)\n",
    "\n",
    "            truth_ratio = np.exp(avg_paraphrased_loss-avg_perturb_loss)\n",
    "            adjusted_truth_ratio = np.minimum(truth_ratio, 1/truth_ratio)\n",
    "            mean_df[eval_task]['truth_ratio'] = np.mean(adjusted_truth_ratio)\n",
    "        else:\n",
    "            avg_paraphrased_loss = df[eval_task]['avg_paraphrased_loss']\n",
    "            avg_perturb_loss = df[eval_task]['average_perturb_loss']\n",
    "\n",
    "            data_indices = list(avg_paraphrased_loss.keys())\n",
    "            \n",
    "            avg_paraphrased_loss = np.array([avg_paraphrased_loss[idx] for idx in data_indices])\n",
    "            avg_perturb_loss = np.array([avg_perturb_loss[idx] for idx in data_indices]).mean(-1)\n",
    "            truth_ratio = np.exp(avg_paraphrased_loss-avg_perturb_loss)\n",
    "            \n",
    "            adjusted_truth_ratio = np.maximum(0, 1-truth_ratio)\n",
    "            mean_df[eval_task]['truth_ratio'] = np.mean(adjusted_truth_ratio)\n",
    "            \n",
    "    return mean_df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forget_quality(unlearn_df, retain_df):\n",
    "    unlearn_df.loc['forget_quality']=np.zeros(len(unlearn_df.columns))\n",
    "    for eval_task, _ in unlearn_df.items():\n",
    "        if eval_task == 'eval_log_forget.json':\n",
    "            retain_truth_ratio = retain_df[eval_task]['truth_ratio']\n",
    "            unlearn_truth_ratio = unlearn_df[eval_task]['truth_ratio']\n",
    "\n",
    "            data_indices = list(retain_truth_ratio.keys())\n",
    "            retain_truth_ratio = np.array([retain_truth_ratio[idx] for idx in data_indices])\n",
    "            unlearn_truth_ratio = np.array([unlearn_truth_ratio[idx] for idx in data_indices])\n",
    "\n",
    "            ks_test = ks_2samp(retain_truth_ratio, unlearn_truth_ratio)\n",
    "            pvalue = ks_test.pvalue\n",
    "            unlearn_df[eval_task]['forget_quality'] = pvalue\n",
    "    return unlearn_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "forget_rates=['forget01', 'forget05', 'forget10']\n",
    "model_family = ['llama2-7b', 'phi']\n",
    "retain_model_path_dict = {\n",
    "    'llama2-7b': {\n",
    "        'forget01': '/home/zhilif/tofu/paper_models/ft_epoch5_lr1e-05_llama2-7b_retain99_wd0.01/checkpoint-618',\n",
    "        'forget05': '/home/zhilif/tofu/paper_models/ft_epoch5_lr1e-05_llama2-7b_retain95_wd0.01/checkpoint-593',\n",
    "        'forget10': '/home/zhilif/tofu/paper_models/ft_epoch5_lr1e-05_llama2-7b_retain90_wd0.01/checkpoint-562'\n",
    "    },\n",
    "    'phi': {\n",
    "        'forget01': '/home/zhilif/tofu/paper_models/ft_epoch5_lr2e-05_phi_retain99_wd0.01/checkpoint-618',\n",
    "        'forget05': '/home/zhilif/tofu/paper_models/ft_epoch5_lr2e-05_phi_retain95_wd0.01/checkpoint-593',\n",
    "        'forget10': '/home/zhilif/tofu/paper_models/ft_epoch5_lr2e-05_phi_retain90_wd0.01/checkpoint-562'\n",
    "    }\n",
    "}\n",
    "retain_df_dict = {}\n",
    "for model in model_family:\n",
    "    retain_df_dict[model] = {}\n",
    "    for rate in forget_rates:\n",
    "        retain_model_path = retain_model_path_dict[model][rate]\n",
    "        retain_df_dict[model][rate] = pd.read_json(join(retain_model_path, f'eval_results/ds_size300/eval_log_aggregated.json'))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_family = ['llama2-7b', 'phi']\n",
    "ft_model_path = {\n",
    "    'llama2-7b': '/home/zhilif/tofu/paper_models/ft_epoch5_lr1e-05_llama2-7b_full_wd0.01/checkpoint-625',\n",
    "    'phi': '/home/zhilif/tofu/paper_models/ft_epoch5_lr2e-05_phi_full_wd0.01/checkpoint-625'\n",
    "}\n",
    "algorithms = ['grad_ascent', 'grad_diff', 'idk', 'KL']\n",
    "forget_rates=['forget01', 'forget05', 'forget10']\n",
    "lr_map = {\n",
    "    'phi': '2e-05',\n",
    "    'llama2-7b': '1e-05',\n",
    "}\n",
    "\n",
    "df_dict = {}\n",
    "for model in model_family:\n",
    "    df_dict[model] = {}\n",
    "    for algo in algorithms:\n",
    "        df_dict[model][algo] = {}\n",
    "        for rate in forget_rates:\n",
    "            ft_df = pd.read_json(join(ft_model_path[model], f'eval_results/ds_size300/eval_log_aggregated.json'))\n",
    "            retain_df = retain_df_dict[model][rate]\n",
    "            ft_df = get_forget_quality(ft_df, retain_df)\n",
    "            mean_ft_df = get_mean_df(ft_df)\n",
    "            subfolder1 = join(ft_model_path[model], f'{algo}_1e-05_{rate}')\n",
    "            # iterate through the subfolder of subfolder1 that starts with checkpoint\n",
    "            ckpt_folders = os.listdir(subfolder1)\n",
    "            ckpt_folders = natsorted([i for i in ckpt_folders if 'checkpoint' in i])\n",
    "            ckpt_df = pd.DataFrame(index=mean_ft_df.index, columns=mean_ft_df.columns)\n",
    "\n",
    "            ckpt_df_list = [mean_ft_df]\n",
    "            for ckpt in ckpt_folders:\n",
    "                eval_result_path = join(subfolder1, ckpt, 'eval_results/ds_size300/eval_log_aggregated.json')\n",
    "                eval_result = pd.read_json(eval_result_path)\n",
    "                eval_result = get_forget_quality(eval_result, retain_df)\n",
    "                mean_df = get_mean_df(eval_result)\n",
    "                ckpt_df_list.append(mean_df)\n",
    "            \n",
    "            for column in ckpt_df.columns:\n",
    "                for index in ckpt_df.index:\n",
    "                    # Concatenate the cell values across DataFrames into a list\n",
    "                    ckpt_df.at[index, column] = [df.at[index, column] for df in ckpt_df_list]\n",
    "\n",
    "            df_dict[model][algo][rate] = ckpt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just some sanity checks\n",
    "# for model in model_family:\n",
    "#     for rate in forget_rates:\n",
    "#         df = retain_df_dict[model][rate]\n",
    "#         for eval_task, metrics in df.items():\n",
    "#             truth_ratio = np.array(list(metrics['truth_ratio'].values()))\n",
    "#             avg_paraphrased_loss = np.array(list(metrics['avg_paraphrased_loss'].values()))\n",
    "#             avg_perturb_loss = np.array(list(metrics['average_perturb_loss'].values())).mean(-1)\n",
    "#             truth_ratio2 = np.exp(avg_paraphrased_loss - avg_perturb_loss)\n",
    "#             assert np.allclose(truth_ratio, truth_ratio2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_step_map = {\n",
    "    'forget01': {\n",
    "        0: 0,\n",
    "        1: 1,\n",
    "        2: 2,\n",
    "        3: 3,\n",
    "        4: 4,\n",
    "        5: 5\n",
    "    },\n",
    "    'forget05': {\n",
    "        0: 0,\n",
    "        1: 6,\n",
    "        2: 12,\n",
    "        3: 18,\n",
    "        4: 24,\n",
    "        5: 30,\n",
    "    },\n",
    "    'forget10': {\n",
    "        0: 0,\n",
    "        1: 12,\n",
    "        2: 24,\n",
    "        3: 36,\n",
    "        4: 48,\n",
    "        5: 60,\n",
    "    }\n",
    "}\n",
    "save_alg_name = {\n",
    "    'grad_ascent': 'grad_ascent',\n",
    "    'grad_diff': 'KL', \n",
    "    'idk': 'dpo',\n",
    "    'KL': 'oracle'\n",
    "}\n",
    "for model in model_family:\n",
    "    for algo in algorithms:\n",
    "        for rate in forget_rates:\n",
    "            try:\n",
    "                ckpt_df = df_dict[model][algo][rate]\n",
    "                    \n",
    "                fig, ax = plt.subplots(1, 3, figsize=(15, 3), sharey=True)\n",
    "\n",
    "                label_names = {\n",
    "                    'rougeL_recall': 'ROUGE',\n",
    "                    'avg_gt_prob': 'Probability',\n",
    "                    'truth_ratio': 'Truth Ratio',\n",
    "                }\n",
    "                forget_rate = float(rate.split('forget')[-1])\n",
    "                retain_rate = 100 - forget_rate\n",
    "                for i, m in enumerate(['rougeL_recall', 'avg_gt_prob', 'truth_ratio']):\n",
    "                    checkpoints = [0, 1, 2, 3 ,4, 5]\n",
    "                    n_ckpts = len(checkpoints)\n",
    "\n",
    "                    if i == 0:\n",
    "                        ax[i].plot(checkpoints, ckpt_df['eval_real_world_wo_options.json'][m][:n_ckpts], label='World Facts', color=colors[2], linewidth=2, markersize=8, marker='o')\n",
    "                        ax[i].plot(checkpoints, ckpt_df['eval_real_author_wo_options.json'][m][:n_ckpts], label='Real Authors', color=colors[0], linewidth=2, markersize=8, marker='x')\n",
    "                        ax[i].plot(checkpoints, ckpt_df['eval_log.json'][m][:n_ckpts], label=f'Retain Set ({retain_rate}%)', color=colors[-1], linewidth=2, markersize=8, marker='*')\n",
    "                        ax[i].plot(checkpoints, ckpt_df['eval_log_forget.json'][m][:n_ckpts], label=f'Forget Set ({int(forget_rate)}%)', color=colors[6], linewidth=2, markersize=8, linestyle='--', marker='^')\n",
    "                    else:\n",
    "                        ax[i].plot(checkpoints, ckpt_df['eval_real_world_wo_options.json'][m][:n_ckpts], color=colors[2], linewidth=2, markersize=8, marker='o')\n",
    "                        ax[i].plot(checkpoints, ckpt_df['eval_real_author_wo_options.json'][m][:n_ckpts], color=colors[0], linewidth=2, markersize=8, marker='x')\n",
    "                        ax[i].plot(checkpoints, ckpt_df['eval_log.json'][m][:n_ckpts], color=colors[-1], linewidth=2, markersize=8, marker='*')\n",
    "                        ax[i].plot(checkpoints, ckpt_df['eval_log_forget.json'][m][:n_ckpts], color=colors[6], linewidth=2, markersize=8, linestyle='--', marker='^')\n",
    "\n",
    "                    ckpt_steps = [str(ckpt_step_map[rate][i]) for i in checkpoints]\n",
    "                    ax[i].set_xticks(checkpoints, ckpt_steps)\n",
    "                    # ax[i].set_xlabel\n",
    "                    ax[i].set_xticks(ax[i].get_xticks()[::2])\n",
    "                    ax[i].set_ylim([-0.1, 1.1])\n",
    "                    if i > 0:\n",
    "                        ax[i].sharey(ax[0])\n",
    "                        # ax[i].set_yticks([])\n",
    "\n",
    "                    \n",
    "                    ax[i].set_ylabel(label_names[m], fontsize=fs)\n",
    "                    \n",
    "                    ax[i].spines['bottom'].set_color('black')\n",
    "                    ax[i].spines['left'].set_color('black')\n",
    "                    ax[i].spines['top'].set_color('black')\n",
    "                    ax[i].spines['right'].set_color('black')\n",
    "                    # ax[1].set_xlabel(f'Unlearning Steps {model, algo, rate}', fontsize=fs)\n",
    "                    ax[1].set_xlabel(f'Unlearning Steps', fontsize=fs)\n",
    "\n",
    "                    fig.legend(loc='upper center', bbox_to_anchor=(0.484, 1.15), ncol=4, fontsize=fs)\n",
    "                    # plt.title(model)\n",
    "                    # plt.xlabel('Training steps')\n",
    "                    # plt.ylabel('ROUGE')\n",
    "                    # plt.legend()\n",
    "                    # plt.show()\n",
    "                    # break\n",
    "                    fig_folder = f'./figure/all_metrics/{model}'\n",
    "                    Path(fig_folder).mkdir(parents=True, exist_ok=True)\n",
    "                    fig.savefig(f'{fig_folder}/1GPU_{save_alg_name[algo]}_1e-05_{rate}_all3metric.pdf', format='pdf', bbox_inches='tight')\n",
    "                    plt.close()\n",
    "            except Exception as e:\n",
    "                print(f'Error in {model} {algo} {rate} ')\n",
    "                print(e)\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
